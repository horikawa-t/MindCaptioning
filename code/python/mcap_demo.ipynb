{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50001a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start script: gpu device:MIG-593efb6c-3714-5470-8214-f4771927764b\n",
      "gpu availability:1\n"
     ]
    }
   ],
   "source": [
    "# filename: mcap_demo.ipynb\n",
    "# source activate mcap_demo\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "rootPath = '/home/psi/horikawa-t/toolbox/public/mcap/'\n",
    "\n",
    "# add path to language project directory\n",
    "sys.path.append('/home/psi/horikawa-t/toolbox/user/python/user/thtoolbox/language/scripts/libraries/mcap/')\n",
    "sys.path.append('/home/psi/horikawa-t/toolbox/user/python/user/thtoolbox/util/code/')\n",
    "from thutil4 import getFN, getDN, setdir, fix_seed,randsample\n",
    "import mcap_utils_demo as mu\n",
    "\n",
    "gpu_use = 1\n",
    "if gpu_use:\n",
    "    gpu_id = 'MIG-593efb6c-3714-5470-8214-f4771927764b'\n",
    "    print('Start script: gpu device:%s' %(gpu_id))\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() and gpu_use == 1 else \"cpu\"\n",
    "print('gpu availability:%d'%(torch.cuda.is_available()))\n",
    "\n",
    "savdir_general = rootPath + 'res/text_generation/'\n",
    "brain_dat_dir_general = rootPath + 'res/decoding/'\n",
    "LMmodeldir = rootPath + 'data/model/'\n",
    "normparam_dat_dir = rootPath + 'data/feature/'\n",
    "capdata_dir = rootPath + 'data/caption/'\n",
    "proxies = {\n",
    "    \"http\": \"\",\n",
    "    \"https\": \"\",\n",
    "}\n",
    "proxies = {\n",
    "    \"http\": \"http://proxy-u.ecl.ntt.co.jp:8080/\",\n",
    "    \"https\": \"http://proxy-u.ecl.ntt.co.jp:8080/\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968d9567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b91d271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mlm model...\n",
      "Loading model and tokenizer from cache...\n",
      "Load roberta-large model done\n",
      "Loading lm model...\n",
      "Loading model and tokenizer from cache...\n",
      "Load roberta-large model done\n"
     ]
    }
   ],
   "source": [
    "# model, normalization, and parameter settings\n",
    "\n",
    "# perform normalization?\n",
    "do_norm = 1\n",
    "\n",
    "# setting model\n",
    "# select MLM from ['bert-base-cased','bert-base-uncased','bert-large-cased','bert-large-uncased','bert-large-uncased-whole-word-masking','bert-large-cased-whole-word-masking','roberta-base','roberta-large','deberta-large-feedback']\n",
    "# you can test untrained model by addiing \"_untrained\" (e.g., 'roberta-large_untrained')\n",
    "MLMType = 'roberta-large' \n",
    "\n",
    "# select LM for feature extraction from ['bert-base-uncased','bert-large-uncased','bert-base-cased','bert-large-cased','bert-large-uncased-whole-word-masking','bert-large-cased-whole-word-masking','openai-gpt','gpt2','gpt2-medium','gpt2-large','gpt2-xl','xlnet-base-cased','xlnet-large-cased','roberta-base','roberta-large','distilbert-base-uncased','distilbert-base-cased','distilgpt2','albert-base-v1','albert-large-v1','albert-xlarge-v1','albert-xxlarge-v1','albert-base-v2','albert-large-v2','albert-xlarge-v2','albert-xxlarge-v2','t5-small','t5-base','t5-large','bart-base','bart-large','ctrl','xlm-mlm-17-1280','xlm-mlm-100-1280','electra','xlm-roberta-base','xlm-roberta-large','clip_l','sgpt','deberta-base','deberta-large','deberta-xlarge']\n",
    "LMType = 'deberta-large'\n",
    "\n",
    "# initialize\n",
    "start = time.time()\n",
    "\n",
    "# load pre-trained masked language model\n",
    "tokenizer, model = mu.load_mlm_model(LMmodeldir, MLMType, proxies, device)\n",
    "# load feature computation model\n",
    "tokenizer_lm, model_lm, nlayers = mu.load_lm_model(LMmodeldir, LMType, proxies, device)\n",
    "# set evaluation mode\n",
    "model.eval(),model_lm.eval()\n",
    "\n",
    "# prepre skip tokens, if any\n",
    "skip_token_ids_mlm = mu.set_skip_token_ids(tokenizer, speficied_skip_tokens=[], include_special_token=True)\n",
    "skip_token_ids_lm = mu.set_skip_token_ids(tokenizer_lm, speficied_skip_tokens=[], include_special_token=True)\n",
    "\n",
    "# load normalization parameters\n",
    "normparam_path = f\"{normparam_dat_dir}/{LMType}/norm_param/\"\n",
    "# normparam_path = f\"/home/ann/project/language/data/features/pytorch//{LMType}/raw/ck20/normParams/tokenmean/ck58/\"\n",
    "feat_mu_all, feat_sd_all = mu.prepare_norm_params(normparam_path, nlayers, device=device) if do_norm else ([],[])\n",
    "\n",
    "# set parameters\n",
    "params = {\n",
    "    'nItr': 100,\n",
    "    'metricType': 'corr',\n",
    "    'do_norm': do_norm,\n",
    "    'beamwidth': 5,\n",
    "    'nMaskCands': 5,\n",
    "    'nMaskPerSentence': 2,\n",
    "    'nGram4Mask': 3,\n",
    "    'multiMaskType':'forward_seq',\n",
    "    'maskingUnitType':'token',\n",
    "    'add_insert_mask': 1,\n",
    "    'mLayerType': 'vstack',\n",
    "    'optimal_th': 0.001,\n",
    "    'topk': 5,\n",
    "    'max_batch_samp': 200,\n",
    "    'length_penalty_type':'token',\n",
    "    'length_penalty_w': 0.10,\n",
    "    'mlmscoreType': 'modified',\n",
    "    'mlm_sampling_type': 'sampling',\n",
    "    'mlms_fix_weight': 0,\n",
    "    'nMax_MLMs_cands':5000,\n",
    "    'do_reflesh': 1,\n",
    "    'reflesh_th': [10,0.1,5,0.00],\n",
    "    'add_mask_removal': False,\n",
    "    'layerIdx': range(0,nlayers),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d045b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23709ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]:<unk>:[score=0.1522, score_reg=0.1522][t=0.03924]\n",
      "[1]:3.:[score=0.2069, score_reg=0.1930][t=0.29230]\n",
      "[2]:5.:[score=0.2272, score_reg=0.2120][t=1.16132]\n",
      "[3]:5.:[score=0.2272, score_reg=0.2120][t=2.65721]\n",
      "[4]:5 Go ahead.:[score=0.2951, score_reg=0.2569][t=4.65430]\n",
      "[5]:Flash the Matrix.:[score=0.3433, score_reg=0.2988][t=6.74760]\n",
      "[6]:I am. Believe me.:[score=0.3964, score_reg=0.3314][t=9.04068]\n",
      "[7]:I Believe You.:[score=0.4066, score_reg=0.3540][t=11.26445]\n",
      "[8]:3 Believe in peace for me.:[score=0.4580, score_reg=0.3770][t=13.45309]\n",
      "[9]:Join me with you.:[score=0.4910, score_reg=0.4180][t=16.11843]\n",
      "[10]:Keep me with you.:[score=0.5288, score_reg=0.4502][t=18.76866]\n",
      "[11]:Rest in your peace with us.:[score=0.5776, score_reg=0.4755][t=21.60732]\n",
      "[12]:Rest in his peace be with you.:[score=0.6348, score_reg=0.5156][t=24.55984]\n",
      "[13]:May peace be with you.:[score=0.8231, score_reg=0.6881][t=27.92304]\n",
      "[14]:May peace be with you.:[score=0.8231, score_reg=0.6881][t=30.39247]\n",
      "[15]:May be with you.:[score=0.8116, score_reg=0.6910][t=33.17444]\n",
      "[16]:May be with you.:[score=0.8116, score_reg=0.6910][t=35.84536]\n",
      "[17]:May be with you.:[score=0.8116, score_reg=0.6910][t=38.40804]\n",
      "[18]:May be with you.:[score=0.8116, score_reg=0.6910][t=40.92137]\n",
      "[19]:May god be with you.:[score=0.8302, score_reg=0.6940][t=43.50697]\n",
      "[20]:May god be with you.:[score=0.8302, score_reg=0.6940][t=46.04085]\n",
      "[21]:May the gods be with you.:[score=0.8705, score_reg=0.7166][t=48.58262]\n",
      "[22]:May the gods be with you.:[score=0.8705, score_reg=0.7166][t=51.22681]\n",
      "[23]:May the gods be with you.:[score=0.8705, score_reg=0.7166][t=53.93740]\n",
      "[24]:May the gods be with you.:[score=0.8705, score_reg=0.7166][t=56.50198]\n",
      "[25]:May the Force be with you.:[score=1.0000, score_reg=0.8232][t=59.01437]\n",
      "Optimized: r > 0.9990000000\n"
     ]
    }
   ],
   "source": [
    "# You can test arbitrary word sequences to examine the effectiveness of our method\n",
    "target_sentence = 'Five apples are on the table.'\n",
    "target_sentence = 'In the beginning God created the heavens and the earth.'\n",
    "target_sentence = 'Imagination is more important than knowledge.'\n",
    "target_sentence = 'To be, or not to be, that is the question.'\n",
    "target_sentence = 'May the Force be with you.'\n",
    "\n",
    "# extract semantic features\n",
    "feat_target, inputs = mu.compute_sentence_feature_patterns_wrapper([target_sentence], model_lm, tokenizer_lm, skip_token_ids=skip_token_ids_lm, do_norm=params['do_norm'], feat_mu_all=feat_mu_all, feat_sd_all=feat_sd_all, device=device, layerIdx=params['layerIdx'], max_batch_samp=params['max_batch_samp'])\n",
    "# Start optimization\n",
    "best_cands, scores_all, scores_eval_all = mu.text_optimization_steps(feat_target[0], feat_mu_all, feat_sd_all, model, tokenizer, skip_token_ids_mlm, model_lm, tokenizer_lm, skip_token_ids_lm, params, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e687b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11cab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce297fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113b7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8f0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8a5b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4e894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a04ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343e00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2fde6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90ae76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b48cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c685e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f769e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45027faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03c2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
